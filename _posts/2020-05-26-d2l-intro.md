---
layout: post
title:  "D2L - Introduction"
date:   2020-05-26T14:25:52-05:00
author: the-realest-one
categories: D2L
tags:	AI D2L
cover:  "/assets/North.jpg"
use_math: true
---

# D2L - Introduction

베이글코드의 데이터 팀은 북리딩이라는 것을 한다. 책을 하나 선정해 2 주마다 일정 분량 읽고, 토론 혹은 발표하는 것이다.

Dive Into Deep Learning 이라는 책이 이번 북리딩 책으로 선정되었다.
그래서 한 챕터를 읽을 때마다 공부 겸 정리 겸 해서 포스팅을 할 예정이다.

영어판 링크: https://d2l.ai/
한글판 링크: https://ko.d2l.ai/

계속해서 업데이트 되고 있는 책이고, 또 링크만 들어가면 무료인 것이 매우 매력적인 책이다. 그런데 깃허브에 의해 한글 번역이 진행되는 것 같다.
그래서 그런지 영판은 19 챕터까지 나와있는데, 한글판은 5 챕터까지 열려있다. 그래서 나는 영어 공부도 할 겸 영판으로 읽어볼 것이다.

책 소개는 이정도로 하고, 바로 본문으로 들어가보자!

## Intro

```
ML is the study of powerful techniques that can learn from experience.
first principle 로 문제를 푸는데 솔루션을 100% 찾을 수 있으면, 머신러닝을 쓰면 "안된다"
```

Programs from the first principle 처럼 정해진 로직만으로 풀기 어려운 문제들이 세상에 많다.
매번 바뀌는 패턴을 추적해야하거나, 오브젝트들간의 관계가 너무나도 복잡할 때가 많다. 그래서, 경험에서 학습해 답을 도출하는 ML 이 필요하다.

## A Motivating Example

아이폰은 어떻게 "시리야" 를 알아채고 일어날 수 있을까? 컴퓨터가 시리를 알아듣게 explicit 하게 코딩하기는 쉽지 않다.
하지만 사람은 가능하다! 사람은 "시리야" 가 뭔지 알고 있으므로, 음성 데이터를 한 번 모아보자.

ML 에서는, 프로그램이 "시리야" 를 알아듣게 explicit 하게 코딩하지 않는다. 여러 ***Parameter*** 들에 의해 작동하는 프로그램을 만든다. 
그리고 이 parameter 들을 바꿔가면서, 우리는 우리의 프로그램을 ***model*** 이라고 한다.
행동이 parameter 에 의해 정해지므로, 각각 다른 parameter 마다 다 다른 program or model 이라 할 수 있다. 이 set 을 ***family of model*** 이라고 한다.
그리고 우리가 모은 데이터셋을 이용해 parameter 를 정하는 프로그램을 ***meta-program*** 혹은 ***learning algorithm*** 이라고 한다.
그리고 ***learning***  이란, 우리의 모델이 원하는 일을 하도록 만드는 parameter 를 찾는 과정이다.

즉, ML 에서는 "시리야" 음성 감지기를 코딩하지 않는다. "시리야" 를 인지하게 학습하는 프로그램을 만든다!

그리고 중요한 이야기를 하는데, 딥러닝에 관한 이야기이다. 딥러닝은 ML 의 방법론 중 하나일 뿐이라는 것.
딥러닝은 많은 계산 레이어를 학습하는 것에서 "deep" 이란 말이 붙었고, 특정 문제들에 기존 ML 방법론들보다 나은 성능을 보인 것이다.
딥러닝은 ML 의 모델을 바꿨을 뿐만 아니라, ML 에 필요했던 feature engineering 의 수고도 줄였다.
그리고, *domain-specific preprocessing* 도 줄였다고 한다(이게 진짜 크다고 생각한다). 언어학 박사가 아니어도, 딥러닝으로 NLP 모델을 만들어볼 수 있는 것이다!

## The Key Components: Data, Models, and Algorithms

책은 4 개의 key component 를 소개한다. data, model, loss function, learning algorithm 이 바로 그것이다. 
각각의 항목에 대해 이야기를 하는데, 나는 그냥 적을 것만 적겠다.

### Objective function

Learning: 어떤 일을 잘해짐. -> 잘해지는 게 뭔데?
우리는 잘하는 게 무엇이고, 우리의 모델이 일을 얼마나 잘하는지 수치화할 필요가 있다. 그래서 정의 하는 것이 ***loss function*** 이다.
loss function 의 값은 모델의 parameter 에 따라 정의되고, 데이터셋에 달라진다.

자. 지금까지 이렇게 4 개를 정의함으로써, "내일 날씨 예측하기", "사진을 보고 어떤 동물인지 분류하기" 등의 문제가 간단해졌고, 수식으로 나타낼 수 있게 되었다.
즉, 컴퓨터에게 시킬 수 있게 된 것이다!

일반적인 프로그래밍으로, "사진을 보고 어떤 동물인지 분류하기" 를 어떻게 코딩할지 나는 감이 잘 안 잡힌다.
하지만 ML 의 세계에서는, "loss 숫자 크기를 줄이는 코드 짜기" 로 단순화되었다.
ML 을 이용하면 나는 데이터 여러 사진을 준비할 것이다. 그리고 사진을 받아 동물분류를 뱉는, parameter 에 따라 달라지는 model 을 정의한다.
model 이 얼마나 잘하는지 못하는지는 loss function 으로 나타낸다.
그러면 우리의 learning algorithm 은, loss function 의 값이 작아지도록 model 의 parameter 를 바꾸는 일을 하면 된다.

## Kind of Machine Learning

### Supervised Learning

우리의 목표는 input vector $\mathbf{x}$ 에 대해, output $f_{\theta}(\mathbf{x})$ 를 매핑하는 모델 $f_\theta$ 를 만드는 것이다.
지도 학습에서 "지도" 라는 말은, parameter 를 고를 때, 데이터와 함께 정답 레이블을 같이 알려주고 학습시키기 때문이다.

통계적인 용어로 볼 때, 보통은 조건부 확률 $P(y \mid x)$ 을 계산하는 것에 관심이 있다.
즉, 사진 x 가 있을 때, 고양이일 확률 $P(cat \mid x)$, 강아지일 확률 $P(dog \mid x)$ 이런 조건부 확률로도 생각해볼 수 있다.

#### Regression

지도학습의 일종. 타겟 값이 특정 범위 내의 임의의 값이 나오는 문제.
우리의 목표는, 예측 target 값들을 실제 target 값들과 근사하게 뱉을 수 있는 모델을 만드는 것이다.
기본적인 Loss function 은 L1 or L2.

`A good rule of thumb is that any How much? or How many? problem should suggest regression.`

L2 loss 는 데이터가 Gaussian noise 에 의해 corrupted 되었다는 가정을 따르고, L1 loss 는 데이터가 Laplace distribution 의 노이즈와 관련이 있다는 가정을 따른다는데....
이건 나중에 다시 찾아봐야겠다. // TODO

#### Classification

지도학습의 일종. Input: feature vector, Output: class among set of options 인 모델을 만드는 것.
기본적인 Loss function 은 cross-entropy or softmax.

`Is that _ ? or What is _?`


#### Tagging

The problem of learning to predict classes that are not mutually exclusive is called ***multi-label classification***.

지도학습의 일종. classification 이랑 비슷해 보이지만, 살짝 다르다. classification 은 한 사진을 보고 어떤 동물인지 맞춰야 하는 것이라면,
tagging 은, 개 고양이 닭 말이 있는 사진을 보고, 개 고양이 닭 말이 있다고 말해야하는 것이다.

#### Search and Ranking

Set 을 찾고, 거기에 또 ranking 을 매겨야 하는 문제. 가장 잘 떠오르는 예시로, 검색 엔진을 들 수 있다.

#### Recommender system

추천 문제는, 관련된 set of items 를 유저에게 보여주는 것이라는 점에서 "Search and Rakning" 문제와 비슷하다.
하지만, 추천 문제는 personalization 에 집중하는 것이 차이점.

예시로 영화 추천 혹은 노래 추천을 들 수 있다.
유저 $u_i$ 와 상품 $p_i$ 에 대해, 예상 별점 $y_{ij}$ 를 예측하는 것이다.
Set of relevant items 를 뽑고, 예상 별점이 높은 아이템들을 유저에게 보여줄 수 있다.

그런데 추천 시스템은, 데이터에 문제가 있을 수 있다. 그냥 저냥 영화를 본 사람은 별점을 남기거나 평가를 남기지 않는다.
보통 엄청 좋아서 5점, 엄청 싫어서 1 점을 남긴다. 이런 *censored feedback* 를 데이터로 쓸 수 밖에 없음을 인지해야한다.
그리고, 이미 해당 유저의 프로덕트에 대한 별점은, 현재 있는 추천 알고리즘의 결과물이다. 즉, biased 되어 있다.

#### Sequence Learning

데이터의 중간 부분이, 앞뒤와 관련이 클 때. eg) 영상, speech 데이터.

Sequence Learning 은 모델이 sequence of input 을 받아 처리하거나, sequence of output 을 뱉도록 한다.

예시로 음성 인식, Text to Speech, 기계 번역 등을 들 수 있다.


### Unsupervised Learning

지도학습과 다르게, target value 들을 주지 않고 학습을 시키는 것이다.
지도학습도 마찬가지이겠지만, 비지도학습에서는 프로젝트의 목적, model 의 input output, 모델의 desired behavior 를 잘 정의하고 시작해야 할 것 같다.

예시:

데이터를 비슷한 것끼리 묶거나, 데이터를 대표하는 몇 개의 prototype 을 뽑는 **Clustering**

데이터의 속성을 정확하게 묘사하는 parameter 를 찾는 **Subspace estimation**. When dependence is linear -> **principal component analysis**.
Dependence 가 linear 하다는 건 무슨 뜻일까? // TODO

속성이 매치되는 유클리드 공간의 오브젝트로 표현이 가능한가? -> **representation learning**. 이것도 뭔 소린지 잘 모르겠다. 찾아보자. // TODO

데이터의 root cause 를 설명할 수 있는가? 설명이 존재하는가? -> **causality and probabilistic graphical models**

실제와 가짜 데이터가 같은지를 체크하며, 데이터를 합성할 수 있게 하는 **generative adversarial networks**

### Interacting with an Environment

지금까지 비지도 학습과 지도 학습에서는 데이터가 어디에서 오고, 모델이 output 을 만들 때 무엇이 일어나는지 신경을 안 썼다.
그냥 데이터 넣고, environment 와는 아무런 상호작용 없이 패턴을 찾거나 예측했음. 그래서, 이런 learning 을 ***offline learning*** 이라고도 부른다.

물론 offline learning 은 환경을 배제함으로써 단순함을 얻는다. 이 단순함은 장점이지만, 문제 서술이 제한된다.
우리가 예측만 하는 "모델"이 아니라, intelligent agent 를 원한다면, 환경을 신경 써야 한다!

예측을 하는 게 아니라 행동을 해야하고, 이런 액션은 환경에 영향을 미친다. 즉, 우리의 프로그램과 환경이 계속해서 상호작용하게 만들고 싶은 것이다.
이럴 때, 환경에 대해 다음과 같은 것들을 생각해볼 수 있다.

* Remember what we did previously?
* Want to help us, e.g., a user reading text into a speech recognizer?
* Want to beat us, i.e., an adversarial setting like spam filtering (against spammers) or playing a game (vs an opponent)?
* Not care (as in many cases)?
* Have shifting dynamics (does future data always resemble the past or do the patterns change over time, either naturally or in response to our automated tools)?

이렇게 환경과의 상호작용을 생각한 ML 기법으로, reinforcement learning 과 adversarial learning 이 있다고 한다.

### Reinforcement learning

환경과 상호작용하고 행동을 하는 agent 를 생각해보기 위해, RL 을 공부해보자.

RL 은 문제를 매우 일반적으로 설명한다. Agent 는 timestep $t$ 에 따라 환경과 상호작용한다.
At each timestep $t$, agent 는 환경으로부터 $o_t$ 를 받는다. 그리고 action $a_t$ 를 선택한다.
이 action 은 환경에 영향을 준다. 마지막으로, agent 는 환경한테 reward $r_t$ 를 받는다.

Agent 는 observatioin -> action 을 매핑하는 함수 ***policy*** 로 대로 행동한다.
`RL 의 목표는 좋은 policy 를 만드는 것이다.`

RL 은 엄청 일반적이다! 모든 지도학습도 RL 문제로 바꿀 수 있다고 한다. 그에 따른 agent 를 설정하고, loss function 과 같은 reward 를 주는 환경을 만들면 되니까.
그리고 지도학습은 training input 이 corret label 과 같이 온다는 가정을 하는데, RL 은 그렇지 않고, 더 일반적으로 reward 를 생각한다.

이외에도, 여러 행동에 의해 결과가 만들어진 것일텐데 결과에만 reward 가 있을 때, 행동들에 credit 을 어떻게 매길지 생각하는 `credit assignment problem`,
현재의 observation 이 현재 상태의 모든 것을 말해주지 않는다는 `partial observability` 를 생각해야 한다.
그리고, 현재의 policy 가 좋지만, 아직 agent  시도 안한 더 좋은 policy 가 있을 수도 있다. 그래서 항상 현재까지의 최고 Policy 를 *exploit* 할지, strategy space 를 *explore* 할지 결정해야한다.

#### MDPs, bandits, and friends

RL 은 너무 복잡해서, 모든 변수들을 다 고려하는 건 너무 힘들다. 그래서, 연구자들은 지금까지 special case 들에 대해 연구하고 있다고 한다.
이렇게 일반적인 문제들에 대해, case 를 제한해 생각하는 사고 방식이 개인적으로 매우 반갑다. 마치 'linear' algebra, 혹은 'real' analysis 같은 느낌.

환경이 완전히 관찰되면 그 RL 문제를 `Markov Decision Process (MDP)` 라고 부르고, state 가 이전 action 과 영향이 없으면 RL 문제를 `contextual bandit problem` 라고 부른다.
그리고 state 가 없어서, initially unknown reward 와 가용 행동 set 만 있을 때는 `multi-armed bandit problem` 라고 한다.

### Conclusion

그 앞의 root, the road to deep learning, success story 는 읽고 싶으면 읽어보도록 하자.
대충 훑는데, 어디에서든지 하는 말이라 나는 그냥 넘겼다ㅋㅋㅋㅋ

Introduction 을 이렇게 길게 정리할 생각이 없었는데, 내가 제대로 모르니 정리도 제대로 안되고, 재밌어서 글이 길어져버렸다.
다음 포스팅은 Preliminaries 이다. ML 을 위해 알아야 할 기본적인 파이썬 기법, 수학들에 대한 챕터인 듯하다. 다음 포스팅도 제대로 정리하면서, 쓸 부분만 써보도록 하겠다.
